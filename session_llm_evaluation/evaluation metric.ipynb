{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'utilities' to the Python path\n",
    "sys.path.append(r'C:\\Users\\Ai Sukmoren\\Desktop\\chatbot')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from utilities.llms import llm\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.tools import YouTubeSearchTool\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "from utilities.llms import llm\n",
    "import os\n",
    "import io\n",
    "import ast\n",
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now you can access these variables using os.getenv\n",
    "url=os.getenv('NE04J_CONN_URL') \n",
    "username=os.getenv('NEO4J_USERNAME') \n",
    "password=os.getenv('NEO4J_PASSWORD')\n",
    "database=os.getenv('NE04J_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'utilities' to the Python path\n",
    "sys.path.append(r'C:\\Users\\Ai Sukmoren\\Desktop\\chatbot')\n",
    "\n",
    "from utilities.llms import llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **G-Eval Overview**: G-Eval is a framework developed to use large language models (LLMs) for evaluating LLM outputs, offering task-specific metrics with better human alignment. It uses chain of thoughts (CoTs) to generate evaluation steps and scores outputs from 1 to 5 based on these steps.\n",
    "\n",
    "- **G-Eval Algorithm**: The algorithm involves introducing an evaluation task to an LLM, defining criteria, generating evaluation steps, creating a prompt, and generating a score. Optionally, probabilities of output tokens can be used for fine-grained scores and minimizing bias.\n",
    "\n",
    "- **Performance**: G-Eval outperforms traditional, non-LLM evaluations by achieving higher alignment with human judgment, demonstrated through higher Spearman and Kendall-Tau correlations. Despite its accuracy, it can still be arbitrary as it relies on LLMs for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f371f143585461aab07f493f4285983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw score: 0.7442219741342808\n",
      "Normalized score (1-5 range): 3.9768878965371233\n",
      "Reason: The sentences generally follow each other logically and maintain a consistent tense and tone. The main idea is clear, but there are a few places where the flow is slightly disrupted.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "# Apply nest_asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Assuming input and output are defined elsewhere in your code\n",
    "input = \"Your input text here\"\n",
    "output = \"Your output text here\"\n",
    "\n",
    "# Create a test case\n",
    "test_case = LLMTestCase(input=input, actual_output=output)\n",
    "\n",
    "# Define the coherence metric\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    criteria=\"Coherence - the collective quality of all sentences in the actual output\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")\n",
    "\n",
    "# Measure the coherence\n",
    "coherence_metric.measure(test_case)\n",
    "\n",
    "# Normalize the score if necessary\n",
    "# Assuming the score should be between 1 and 5\n",
    "normalized_score = coherence_metric.score * 4 + 1\n",
    "\n",
    "# Print the results\n",
    "print(\"Raw score:\", coherence_metric.score)\n",
    "print(\"Normalized score (1-5 range):\", normalized_score)\n",
    "print(\"Reason:\", coherence_metric.reason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      "What is the top 3 highest anime rating where rating is not null\n",
      "Full Context:\n",
      "[{'a.name': 'taka no tsume 8: yoshida-kun no x-files', 'a.rating': 10.0}, {'a.name': 'mogura no motoro', 'a.rating': 9.5}, {'a.name': 'kimi no na wa.', 'a.rating': 9.37}]\n",
      "Output:\n",
      "The top 3 highest-rated anime where the rating is not null are \"taka no tsume 8: yoshida-kun no x-files\" with a rating of 10.0, \"mogura no motoro\" with a rating of 9.5, and \"kimi no na wa.\" with a rating of 9.37.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the Neo4j graph connection\n",
    "graph = Neo4jGraph(url=os.getenv('NE04J_CONN_URL'), \n",
    "                    username=os.getenv('NEO4J_USERNAME'), \n",
    "                    password=os.getenv('NEO4J_PASSWORD'),\n",
    "                    database=os.getenv('NE04J_DB'))\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = \"\"\"\n",
    "You are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\n",
    "Convert the user's question based on the schema.\n",
    "\n",
    "instruction:\n",
    "- all animes is in lower cases\n",
    "- all nodes include Anime and Genre\n",
    "\n",
    "Varibales:\n",
    "Schema {schema},\n",
    "Question {question}\n",
    "\"\"\"\n",
    "\n",
    "cypher_generation_prompt = PromptTemplate(\n",
    "    template=prompt,\n",
    "    input_variables=[\"schema\", \"question\"],\n",
    ")\n",
    "\n",
    "# Create the GraphCypherQAChain instance\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define the question\n",
    "question = \"What is the top 3 highest anime rating where rating is not null\"\n",
    "\n",
    "# Capture the verbose output\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = io.StringIO()\n",
    "\n",
    "# Invoke the chain with the question\n",
    "res = cypher_chain.invoke(question)\n",
    "\n",
    "# Get the verbose output\n",
    "verbose_output = sys.stdout.getvalue()\n",
    "sys.stdout = old_stdout\n",
    "\n",
    "# Function to extract Full Context\n",
    "def extract_full_context(text):\n",
    "    match = re.search(r'Full Context:\\n\\x1b\\[32;1m\\x1b\\[1;3m(.*?)\\x1b\\[0m', text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"Full Context not found.\"\n",
    "\n",
    "# Extract and print the Full Context\n",
    "full_context = extract_full_context(verbose_output)\n",
    "print(\"question:\")\n",
    "print(res['query'])\n",
    "print(\"Full Context:\")\n",
    "print(full_context)\n",
    "print(\"Output:\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Use Case Specific Metrics**: The choice of LLM evaluation metrics depends on the specific use case and architecture of your LLM application, such as using RAG metrics like Faithfulness and Answer Relevancy for a customer support chatbot.\n",
    "\n",
    "- **Fine-Tuning Metrics**: When fine-tuning your own models, such as Mistral 7B, you need to consider metrics like bias to ensure impartial and fair LLM decisions.\n",
    "\n",
    "- **Essential Metrics**: It's crucial to understand and implement the necessary evaluation metrics tailored to your LLM's purpose, ensuring optimal performance and reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Faithfulness\n",
    "\n",
    "- **Faithfulness Metric**: Evaluates if the LLM's outputs in a RAG pipeline factually align with the information in the retrieval context. The QAG Scorer is recommended for this metric.\n",
    "\n",
    "- **QAG Scorer Algorithm**: Extract all claims from the LLM output, verify each claim against the retrieval context nodes, and use closed-ended questions to check agreement. Calculate faithfulness by dividing the number of truthful claims ('yes' and 'idk') by the total number of claims.\n",
    "\n",
    "- **Accuracy and Reliability**: This method leverages LLM's reasoning capabilities and ensures accurate evaluation, avoiding the unreliability of LLM-generated scores, making it superior to methods like G-Eval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (a:anime)\n",
      "WHERE a.rating IS NOT NULL\n",
      "RETURN a.name AS anime_name, a.rating AS rating\n",
      "ORDER BY a.rating DESC\n",
      "LIMIT 3\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'anime_name': 'taka no tsume 8: yoshida-kun no x-files', 'rating': 10.0}, {'anime_name': 'mogura no motoro', 'rating': 9.5}, {'anime_name': 'kimi no na wa.', 'rating': 9.37}]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c46225a96a24e2aa2bfd444883dd495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "The score is 1.00 because there are no contradictions, which indicates perfect alignment with the retrieval context. Great job!\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Define the question\n",
    "question = \"What is the top 3 highest anime rating where rating is not null\"\n",
    "res = cypher_chain.invoke(question)\n",
    "full_context = extract_full_context(verbose_output)\n",
    "\n",
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "test_case=LLMTestCase(\n",
    "  input=question, \n",
    "  actual_output=res['result'],\n",
    "  retrieval_context=[full_context]\n",
    ")\n",
    "metric = FaithfulnessMetric(threshold=0.5)\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "print(metric.is_successful())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Relevancy\n",
    "\n",
    "- **Answer Relevancy Metric**: This RAG metric assesses if the generator outputs concise and relevant answers by calculating the proportion of relevant sentences in the LLM output.\n",
    "\n",
    "- **Calculation Method**: Determine answer relevancy by dividing the number of relevant sentences by the total number of sentences, considering the retrieval context to justify relevancy.\n",
    "\n",
    "- **Implementation**: Utilize the `AnswerRelevancyMetric` from the `deepeval.metrics` library to build a robust answer relevancy evaluation, incorporating additional context as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (a:anime)\n",
      "WHERE a.rating IS NOT NULL\n",
      "RETURN a.name, a.rating\n",
      "ORDER BY a.rating DESC\n",
      "LIMIT 3\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'a.name': 'taka no tsume 8: yoshida-kun no x-files', 'a.rating': 10.0}, {'a.name': 'mogura no motoro', 'a.rating': 9.5}, {'a.name': 'kimi no na wa.', 'a.rating': 9.37}]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5433fd0783e4d7b9d3731b7a8a36d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "The score is 1.00 because the output perfectly addresses the input without any irrelevant statements. Great job!\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Define the question\n",
    "question = \"What is the top 3 highest anime rating where rating is not null\"\n",
    "res = cypher_chain.invoke(question)\n",
    "full_context = extract_full_context(verbose_output)\n",
    "\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "test_case=LLMTestCase(\n",
    "  input=question, \n",
    "  actual_output=res['result'],\n",
    "  retrieval_context=[full_context]\n",
    ")\n",
    "metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "print(metric.is_successful())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Relevancy\n",
    "\n",
    "- **Contextual Relevancy Metric**: Measures the proportion of sentences in the retrieval context that are relevant to the given input.\n",
    "\n",
    "- **Calculation Method**: Determine contextual relevancy by dividing the number of relevant sentences in the retrieval context by the total number of sentences.\n",
    "\n",
    "- **Simplicity and Clarity**: This metric provides a straightforward way to assess the alignment of retrieval context with the input, ensuring concise and pertinent information retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (a:anime)\n",
      "WHERE a.rating IS NOT NULL\n",
      "RETURN a.name, a.rating\n",
      "ORDER BY a.rating DESC\n",
      "LIMIT 3\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'a.name': 'taka no tsume 8: yoshida-kun no x-files', 'a.rating': 10.0}, {'a.name': 'mogura no motoro', 'a.rating': 9.5}, {'a.name': 'kimi no na wa.', 'a.rating': 9.37}]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bef70a86c1543e9bdb557f2124319a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "The score is 1.00 because everything looks perfect! Great job!\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Define the question\n",
    "question = \"What is the top 3 highest anime rating where rating is not null\"\n",
    "res = cypher_chain.invoke(question)\n",
    "full_context = extract_full_context(verbose_output)\n",
    "\n",
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "test_case=LLMTestCase(\n",
    "  input=question, \n",
    "  actual_output=res['result'],\n",
    "  retrieval_context=[full_context]\n",
    ")\n",
    "metric = ContextualRelevancyMetric(threshold=0.5)\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "print(metric.is_successful())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
