{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'utilities' to the Python path\n",
    "sys.path.append(r'C:\\Users\\Ai Sukmoren\\Desktop\\chatbot')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from utilities.llms import llm\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.tools import YouTubeSearchTool\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "from utilities.llms import llm\n",
    "import os\n",
    "import io\n",
    "import ast\n",
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now you can access these variables using os.getenv\n",
    "url=os.getenv('NE04J_CONN_URL') \n",
    "username=os.getenv('NEO4J_USERNAME') \n",
    "password=os.getenv('NEO4J_PASSWORD')\n",
    "database=os.getenv('NE04J_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def kg_gen(query):\n",
    "    # Initialize the Neo4j graph connection\n",
    "    graph = Neo4jGraph(url=os.getenv('NE04J_CONN_URL'), \n",
    "                        username=os.getenv('NEO4J_USERNAME'), \n",
    "                        password=os.getenv('NEO4J_PASSWORD'),\n",
    "                        database=os.getenv('NE04J_DB'))\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt = \"\"\"\n",
    "    You are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\n",
    "    Convert the user's question based on the schema.\n",
    "\n",
    "    instruction:\n",
    "    - all animes is in lower cases\n",
    "    - all nodes include Anime and Genre\n",
    "\n",
    "    Varibales:\n",
    "    Schema {schema},\n",
    "    Question {question}\n",
    "    \"\"\"\n",
    "\n",
    "    cypher_generation_prompt = PromptTemplate(\n",
    "        template=prompt,\n",
    "        input_variables=[\"schema\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Create the GraphCypherQAChain instance\n",
    "    cypher_chain = GraphCypherQAChain.from_llm(\n",
    "        llm=llm,\n",
    "        graph=graph,\n",
    "        cypher_prompt=cypher_generation_prompt,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # Capture the verbose output\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = io.StringIO()\n",
    "\n",
    "    # Invoke the chain with the question\n",
    "    res = cypher_chain.invoke(question)\n",
    "\n",
    "    # Get the verbose output\n",
    "    verbose_output = sys.stdout.getvalue()\n",
    "    sys.stdout = old_stdout\n",
    "\n",
    "    # Function to extract Full Context\n",
    "    def extract_full_context(text):\n",
    "        match = re.search(r'Full Context:\\n\\x1b\\[32;1m\\x1b\\[1;3m(.*?)\\x1b\\[0m', text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return \"Full Context not found.\"\n",
    "    \n",
    "    full_context = extract_full_context(verbose_output)\n",
    "    \n",
    "    return res,full_context\n",
    "\n",
    "# Define the question\n",
    "question = \"What is the top 3 highest anime rating where rating is not null\"\n",
    "\n",
    "# Extract and print the Full Context\n",
    "res,full_context = kg_gen(question)\n",
    "\n",
    "print(\"question:\")\n",
    "print(res['query'])\n",
    "print(\"Full Context:\")\n",
    "print(full_context)\n",
    "print(\"Output:\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G-eval\n",
    "\n",
    "- **G-Eval Overview**: G-Eval is a framework developed to use large language models (LLMs) for evaluating LLM outputs, offering task-specific metrics with better human alignment. It uses chain of thoughts (CoTs) to generate evaluation steps and scores outputs from 1 to 5 based on these steps.\n",
    "\n",
    "- **G-Eval Algorithm**: The algorithm involves introducing an evaluation task to an LLM, defining criteria, generating evaluation steps, creating a prompt, and generating a score. Optionally, probabilities of output tokens can be used for fine-grained scores and minimizing bias.\n",
    "\n",
    "- **Performance**: G-Eval outperforms traditional, non-LLM evaluations by achieving higher alignment with human judgment, demonstrated through higher Spearman and Kendall-Tau correlations. Despite its accuracy, it can still be arbitrary as it relies on LLMs for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "import nest_asyncio\n",
    "# Apply nest_asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Assuming input and output are defined elsewhere in your code\n",
    "input = \"Your input text here\"\n",
    "output = \"Your output text here\"\n",
    "\n",
    "# Create a test case\n",
    "test_case = LLMTestCase(input=input, actual_output=output)\n",
    "\n",
    "# Define the coherence metric\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    criteria=\"Coherence - the collective quality of all sentences in the actual output\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")\n",
    "\n",
    "# Measure the coherence\n",
    "coherence_metric.measure(test_case)\n",
    "\n",
    "# Normalize the score if necessary\n",
    "# Assuming the score should be between 1 and 5\n",
    "normalized_score = coherence_metric.score * 4 + 1\n",
    "\n",
    "# Print the results\n",
    "print(\"Raw score:\", coherence_metric.score)\n",
    "print(\"Normalized score (1-5 range):\", normalized_score)\n",
    "print(\"Reason:\", coherence_metric.reason)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Faithfulness\n",
    "\n",
    "- **Faithfulness Metric**: Evaluates if the LLM's outputs in a RAG pipeline factually align with the information in the retrieval context. The QAG Scorer is recommended for this metric.\n",
    "\n",
    "- **QAG Scorer Algorithm**: Extract all claims from the LLM output, verify each claim against the retrieval context nodes, and use closed-ended questions to check agreement. Calculate faithfulness by dividing the number of truthful claims ('yes' and 'idk') by the total number of claims.\n",
    "\n",
    "- **Accuracy and Reliability**: This method leverages LLM's reasoning capabilities and ensures accurate evaluation, avoiding the unreliability of LLM-generated scores, making it superior to methods like G-Eval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the question\n",
    "question = \"What is the top 3 highest anime rating where rating is not null\"\n",
    "res = cypher_chain.invoke(question)\n",
    "full_context = extract_full_context(verbose_output)\n",
    "\n",
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "test_case=LLMTestCase(\n",
    "  input=question, \n",
    "  actual_output=res['result'],\n",
    "  retrieval_context=[full_context]\n",
    ")\n",
    "metric = FaithfulnessMetric(threshold=0.5)\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "print(metric.is_successful())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Relevancy\n",
    "\n",
    "- **Answer Relevancy Metric**: This RAG metric assesses if the generator outputs concise and relevant answers by calculating the proportion of relevant sentences in the LLM output.\n",
    "\n",
    "- **Calculation Method**: Determine answer relevancy by dividing the number of relevant sentences by the total number of sentences, considering the retrieval context to justify relevancy.\n",
    "\n",
    "- **Implementation**: Utilize the `AnswerRelevancyMetric` from the `deepeval.metrics` library to build a robust answer relevancy evaluation, incorporating additional context as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the question\n",
    "question = \"What is the top 3 highest anime rating where rating is not null\"\n",
    "res = cypher_chain.invoke(question)\n",
    "full_context = extract_full_context(verbose_output)\n",
    "\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "test_case=LLMTestCase(\n",
    "  input=question, \n",
    "  actual_output=res['result'],\n",
    "  retrieval_context=[full_context]\n",
    ")\n",
    "metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "print(metric.is_successful())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Relevancy\n",
    "\n",
    "- **Contextual Relevancy Metric**: Measures the proportion of sentences in the retrieval context that are relevant to the given input.\n",
    "\n",
    "- **Calculation Method**: Determine contextual relevancy by dividing the number of relevant sentences in the retrieval context by the total number of sentences.\n",
    "\n",
    "- **Simplicity and Clarity**: This metric provides a straightforward way to assess the alignment of retrieval context with the input, ensuring concise and pertinent information retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the question\n",
    "question = \"What is the top 3 highest anime rating where rating is not null\"\n",
    "res = cypher_chain.invoke(question)\n",
    "full_context = extract_full_context(verbose_output)\n",
    "\n",
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "test_case=LLMTestCase(\n",
    "  input=question, \n",
    "  actual_output=res['result'],\n",
    "  retrieval_context=[full_context]\n",
    ")\n",
    "metric = ContextualRelevancyMetric(threshold=0.5)\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "print(metric.is_successful())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
